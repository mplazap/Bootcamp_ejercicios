{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARTE 1 (10 %) Carga de datos de diamonds desde CSV con schema: https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/diamonds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, IntegerType, NumericType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import StringIndexer, Imputer, OneHotEncoder, VectorAssembler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- carat: float (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: float (nullable = true)\n",
      " |-- table: float (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      " |-- z: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('diamonds_evaluate').getOrCreate()\n",
    "\n",
    "# Cargamos el CSV y hacemos el Schema\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/diamonds.csv'\n",
    "csv_path = 'diamonds.csv'\n",
    "\n",
    "with open(csv_path, 'wb') as file:\n",
    "    file.write(requests.get(url).content)\n",
    "    \n",
    "schema = StructType([\n",
    "    StructField('carat', FloatType(), True),\n",
    "    StructField('cut', StringType(), True),\n",
    "    StructField('color', StringType(), True),\n",
    "    StructField('clarity', StringType(), True),\n",
    "    StructField('depth', FloatType(), True),\n",
    "    StructField('table', FloatType(), True),\n",
    "    StructField('price', IntegerType(), True),\n",
    "    StructField('x', FloatType(), True),\n",
    "    StructField('y', FloatType(), True),\n",
    "    StructField('z', FloatType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=False, schema=schema)\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARTE 2 (40 %) Pipeline regresión price con preprocesados\n",
    "  * Imputer, StringIndexer, OneHotEncoder, MinMaxScaler o StandardScaler, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|carat|cut|color|clarity|depth|table|price|  x|  y|  z|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|    0|  0|    0|      0|    0|    0|    0|  0|  0|  0|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Como vamos a predecir 'Price' borramos filas donde 'Price' sea nan:\n",
    "df_reg = df.dropna(subset=['price'])\n",
    "\n",
    "# Contamos nulos en todas las columnas: equivalente a pandas df.isna().sum()\n",
    "df_reg.select([sum(col(c).isNull().cast('int')).alias(c) for c in df_reg.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|label|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renombramos la columna 'Price' como 'Label'\n",
    "df_reg = df_reg.withColumnRenamed('price', 'label')\n",
    "df_reg.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat', 'depth', 'table', 'label', 'x', 'y', 'z']\n",
      "['cut', 'color', 'clarity']\n"
     ]
    }
   ],
   "source": [
    "# Separamos las columnas a las que aplicar preprocesados\n",
    "numerical_columns = [field.name for field in df_reg.schema.fields if isinstance(field.dataType, NumericType) and field.name != 'price']\n",
    "categorical_columns = [field.name for field in df_reg.schema.fields if isinstance(field.dataType, StringType)]\n",
    "label_col = 'price'\n",
    "\n",
    "print(numerical_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed', 'color_indexed', 'clarity_indexed']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos el Indexers para las columnas(features) categoricas de la entrada\n",
    "# Crea un objeto StringIndexer por cada columna\n",
    "indexers_features = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + '_indexed', handleInvalid='keep') for c in categorical_columns\n",
    "]\n",
    "categorical_columns_indexed = [c + '_indexed' for c in categorical_columns]\n",
    "\n",
    "print(categorical_columns_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed_imputed', 'color_indexed_imputed', 'clarity_indexed_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos el Imputer con la moda para las columnas categoricas indexadas\n",
    "imputer_categorical = Imputer(\n",
    "    inputCols=categorical_columns_indexed,\n",
    "    outputCols=[c + '_imputed' for c in categorical_columns_indexed],\n",
    "    strategy='mode'\n",
    ")\n",
    "categorical_cols_indexed_imputed = [c + '_imputed' for c in categorical_columns_indexed]\n",
    "\n",
    "print(categorical_cols_indexed_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed_imputed_onehot', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos el OneHotEncoder para las columnas categoricas \n",
    "encoders_onehot = [\n",
    "    OneHotEncoder(inputCol=c, outputCol=c + '_onehot') \n",
    "    for c in categorical_cols_indexed_imputed\n",
    "]\n",
    "categorical_cols_onehot = [c + '_onehot' for c in categorical_cols_indexed_imputed]\n",
    "\n",
    "print(categorical_cols_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'label_imputed', 'x_imputed', 'y_imputed', 'z_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos el Imputer a las columnas numericas\n",
    "imputer_numerical = Imputer(\n",
    "    inputCols=numerical_columns,\n",
    "    outputCols=[c + '_imputed' for c in numerical_columns],\n",
    "    strategy='median'\n",
    ")\n",
    "numerical_cols_imputed = [c + '_imputed' for c in numerical_columns]\n",
    "\n",
    "print(numerical_cols_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos columnas numericas con MinMaxScaler\n",
    "assembler_numerical = VectorAssembler(\n",
    "    inputCols=numerical_cols_imputed,\n",
    "    outputCol='numeric_features'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='numeric_features',\n",
    "    outputCol='numeric_features_scaled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['numeric_features_scaled', 'cut_indexed_imputed_onehot', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Juntamos todas las columnas\n",
    "all_columns = ['numeric_features_scaled'] + categorical_cols_onehot\n",
    "\n",
    "print(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblamos las columnas numéricas + categóricas y obtenemos features\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=all_columns,\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el Regressor \n",
    "regressor = RandomForestRegressor(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el particionamiento de datos\n",
    "df_train, df_test = df_reg.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el PipeLine\n",
    "pipeline = Pipeline(stages = [\n",
    "    # Hacemos el Indexers para las columnas categóricas \n",
    "    *indexers_features, # Ponemos * porque es una lista de objetos\n",
    "    # Hacemos el Imputer para las columnas categóricas\n",
    "    imputer_categorical,\n",
    "    # Hacemos el OneHotEncoders para columnas categóricas\n",
    "    *encoders_onehot,\n",
    "    # Hacemos el Imputer para columnas numéricas\n",
    "    imputer_numerical,\n",
    "    # Ensamblamos columnas numéricas + escalado\n",
    "    assembler_numerical,\n",
    "    scaler,\n",
    "    # Ensamblamos columnas numéricas escaladas + columnas categóricas en una sola columna 'features'\n",
    "    assembler_all,\n",
    "    # 8. Modelo\n",
    "    regressor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el PipeLine\n",
    "pipeline_model = pipeline.fit(df_train)\n",
    "df_pred = pipeline_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.983\n",
      "mae: 303.824\n",
      "mse: 280472.219\n",
      "rmse: 529.596\n"
     ]
    }
   ],
   "source": [
    "# Creamos los evaluadores\n",
    "evaluator_r2 = RegressionEvaluator(metricName='r2')\n",
    "evaluator_mae = RegressionEvaluator(metricName='mae')\n",
    "evaluator_mse = RegressionEvaluator(metricName='mse')\n",
    "evaluator_rmse = RegressionEvaluator(metricName='rmse')\n",
    "\n",
    "print(f\"r2: {evaluator_r2.evaluate(df_pred):.3f}\")\n",
    "print(f\"mae: {evaluator_mae.evaluate(df_pred):.3f}\")\n",
    "print(f\"mse: {evaluator_mse.evaluate(df_pred):.3f}\")\n",
    "print(f\"rmse: {evaluator_rmse.evaluate(df_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARTE 3 (40 %) Pipeline clasificación multiclase sobre variable cut con preprocesados\n",
    "  * Imputer, StringIndexer, OneHotEncoder, MinMaxScaler o StandardScaler, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|carat|cut|color|clarity|depth|table|price|  x|  y|  z|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|    0|  0|    0|      0|    0|    0|    0|  0|  0|  0|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Como vamos a predecir la columna 'Cut' borramos filas donde 'Cut' sea nan:\n",
    "df_class = df.dropna(subset=['cut'])\n",
    "\n",
    "# Contamos nulos en todas las columnas: equivalente a pandas df.isna().sum()\n",
    "df_class.select([sum(col(c).isNull().cast('int')).alias(c) for c in df_class.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las columnas a las que aplicar preprocesados\n",
    "numerical_columns = [field.name for field in df_class.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "categorical_columns = [field.name for field in df_class.schema.fields if isinstance(field.dataType, StringType) and field.name != 'cut']\n",
    "label_col = 'cut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos Indexer para la columna 'Cut'\n",
    "indexer_label = StringIndexer(\n",
    "    inputCol=label_col, # Indexa columna 'Cut', que es la que queremos predecir\n",
    "    outputCol='label',\n",
    "    handleInvalid='keep'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed', 'clarity_indexed']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos Indexer para las columnas(features) de la entrada\n",
    "# Crea un objeto StringIndexer por cada columna categórica a indexar\n",
    "indexers_features = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + '_indexed', handleInvalid='keep') for c in categorical_columns\n",
    "]\n",
    "categorical_cols_indexed = [c + '_indexed' for c in categorical_columns]\n",
    "\n",
    "print(categorical_cols_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed_imputed', 'clarity_indexed_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos Imputer a las columnas categoricas\n",
    "imputer_categorical = Imputer(\n",
    "    inputCols=categorical_cols_indexed,\n",
    "    outputCols=[c + '_imputed' for c in categorical_cols_indexed],\n",
    "    strategy='mode'\n",
    ")\n",
    "categorical_cols_indexed_imputed = [c + '_imputed' for c in categorical_cols_indexed]\n",
    "\n",
    "print(categorical_cols_indexed_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos OneHotEncoder a als columnas categoricas\n",
    "encoders_onehot = [\n",
    "    OneHotEncoder(inputCol=c, outputCol=c + '_onehot') \n",
    "    for c in categorical_cols_indexed_imputed\n",
    "]\n",
    "categorical_cols_onehot = [c + '_onehot' for c in categorical_cols_indexed_imputed]\n",
    "\n",
    "print(categorical_cols_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'price_imputed', 'x_imputed', 'y_imputed', 'z_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Hacemos Imputer a las columnas numericas\n",
    "imputer_numerical = Imputer(\n",
    "    inputCols=numerical_columns,\n",
    "    outputCols=[c + '_imputed' for c in numerical_columns],\n",
    "    strategy='median'\n",
    ")\n",
    "numerical_cols_imputed = [c + '_imputed' for c in numerical_columns]\n",
    "\n",
    "print(numerical_cols_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos las columnas numéricas con MinMaxScaler\n",
    "assembler_numerical = VectorAssembler(\n",
    "    inputCols=numerical_cols_imputed,\n",
    "    outputCol='numeric_features'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='numeric_features',\n",
    "    outputCol='numeric_features_scaled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['numeric_features_scaled', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# JUntamos todas las columnas\n",
    "all_columns = ['numeric_features_scaled'] + categorical_cols_onehot\n",
    "\n",
    "print(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblamos las columnas numéricas + categóricas y obtenemos features\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=all_columns,\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el clasificador\n",
    "classifier = RandomForestClassifier(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el particionamiento\n",
    "df_train, df_test = df_class.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [\n",
    "    # Hacemos Indexer para 'cut' que es la columna a predecir\n",
    "    indexer_label,\n",
    "    # Hacemos Indexer para columnas categóricas\n",
    "    *indexers_features, # ponemos * porque es una lista de objetos\n",
    "    # Hacemos Imputer para columnas categóricas\n",
    "    imputer_categorical,\n",
    "    # Hacemo OneHotEncoders para columnas categóricas\n",
    "    *encoders_onehot,\n",
    "    # Hacemos Imputer para columnas numéricas\n",
    "    imputer_numerical,\n",
    "    # Ensamblamos columnas numéricas y hacemos escalado\n",
    "    assembler_numerical,\n",
    "    scaler,\n",
    "    # Ensamblamos numéricas escaladas y categóricas en la columna 'features'\n",
    "    assembler_all,\n",
    "    # Hacemos el modelo de clasificación\n",
    "    classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el PipeLine\n",
    "pipeline_model = pipeline.fit(df_train)\n",
    "df_pred = pipeline_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.669\n",
      "f1: 0.619\n",
      "precision: 0.647\n",
      "recall: 0.669\n"
     ]
    }
   ],
   "source": [
    "# Creamos los evaluadores\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(metricName='f1')\n",
    "evaluator_precision = MulticlassClassificationEvaluator(metricName='weightedPrecision')\n",
    "evaluator_recall = MulticlassClassificationEvaluator(metricName='weightedRecall')\n",
    "\n",
    "print(f\"accuracy: {evaluator_accuracy.evaluate(df_pred):.3f}\")\n",
    "print(f\"f1: {evaluator_f1.evaluate(df_pred):.3f}\")\n",
    "print(f\"precision: {evaluator_precision.evaluate(df_pred):.3f}\")\n",
    "print(f\"recall: {evaluator_recall.evaluate(df_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARTE 4 (10 %) Gridsearch con CrossValidation sobre cualquiera de los pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos, se puede utilizar RandomForest para los dos por ejemplo o el que se quiera. Ejemplo RandomForestRegressor para regresión y MultiLayerPerceptronClassifier para clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el Gridsearch\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(classifier.numTrees, [15, 20, 25, 30]) # Por defecto es 20\n",
    "    .addGrid(classifier.maxDepth, [1, 3, 5, 10, 15, 20]) # Por defecto es 5(Tiene un rango de [0, 30])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamo sel CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid, # Parámetros para grid search hyper parameter tuning\n",
    "    evaluator=evaluator_f1,\n",
    "    numFolds=3, # Valor por defecto\n",
    "    parallelism=4,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "cv_model = crossval.fit(df_train)\n",
    "df_pred = cv_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.721\n",
      "f1: 0.707\n",
      "precision: 0.706\n",
      "recall: 0.721\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las metricas\n",
    "print(f\"accuracy: {evaluator_accuracy.evaluate(df_pred):.3f}\")\n",
    "print(f\"f1: {evaluator_f1.evaluate(df_pred):.3f}\")\n",
    "print(f\"precision: {evaluator_precision.evaluate(df_pred):.3f}\")\n",
    "print(f\"recall: {evaluator_recall.evaluate(df_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
